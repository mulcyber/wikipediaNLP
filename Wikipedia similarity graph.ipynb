{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia similarity graph\n",
    "\n",
    "The goal of this notebook is to create a graph where node are wikipedia page and weighted edges are the similarity between articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wikipedia_preprocessing as wiki\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from os import path\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "DATA_folder = path.join(\"Data\", \"Wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing similarity\n",
    "\n",
    "### Step 1: Fetching wikipedia pages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  exists: aborting download (use force=True to download anyway).\n",
      "Pages already precomputed: aborting (use force=True).\n"
     ]
    }
   ],
   "source": [
    "wiki.get_all_dumps()\n",
    "wiki.precompute_pages(max_dumps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_category = re.compile(\"^\\s*Category:.*\\s*$\")\n",
    "reg_redirect = re.compile(\"REDIRECT\")\n",
    "\n",
    "def get_categories(page):\n",
    "    categories = []\n",
    "    for line in page[\"content\"].split(\"\\n\"):\n",
    "        if reg_redirect.match(line):\n",
    "            return []\n",
    "        if reg_category.match(line):\n",
    "            categories.append(line.strip())\n",
    "    return categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_categories(pages):\n",
    "    categories = {}\n",
    "    i = 0\n",
    "    for p in pages:\n",
    "        print(\"\\r%s articles computed\" % i, end=\"\")\n",
    "        i += 1\n",
    "        page_cat = get_categories(p)\n",
    "        for cat in page_cat:\n",
    "            if cat in categories:\n",
    "                categories[cat].append(p[\"name\"])\n",
    "            else:\n",
    "                categories[cat] = [p[\"name\"]]\n",
    "    remove = [cat for cat, l in categories.items() if len(l) < 2]\n",
    "    for cat in remove:\n",
    "        del categories[cat]\n",
    "    print(\"\\nRemoved %d categories with 1 article\" % len(remove))\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19821 articles computed\n",
      "Removed 43798 categories with 1 article\n",
      "2480 articles kept\n"
     ]
    }
   ],
   "source": [
    "pages = wiki.Pages()\n",
    "cats = all_categories(pages)\n",
    "goodcat = [cat for cat, l in cats.items() if len(l) > 50]\n",
    "\n",
    "def get_connected_articles(cats, goodcats):\n",
    "    articles = set()\n",
    "    for cat in goodcats:\n",
    "        for art in cats[cat]:\n",
    "            articles.add(art)\n",
    "    return list(articles)\n",
    "\n",
    "def reorder_names(names):\n",
    "    pages = wiki.Pages()\n",
    "    new_names = []\n",
    "    for p in pages:\n",
    "        if p[\"name\"] in names:\n",
    "            new_names.append(p[\"name\"])\n",
    "    return new_names\n",
    "\n",
    "    \n",
    "names=reorder_names(get_connected_articles(cats, goodcat))\n",
    "len(names)\n",
    "Ncorp = len(names)\n",
    "print(\"%d articles kept\" % Ncorp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating reference matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cat_matrix(goodcats, names):\n",
    "    Ncorp = len(names)\n",
    "    cat_matrix = np.zeros((Ncorp, Ncorp))\n",
    "    for cat in goodcats:\n",
    "        for i in range(len(cats[cat])):\n",
    "            for j in range(1, i):\n",
    "                x = names.index(cats[cat][i])\n",
    "                y = names.index(cats[cat][j])\n",
    "                cat_matrix[x, y] = cat_matrix[y, x] = 1\n",
    "    return cat_matrix\n",
    "\n",
    "cat_matrix = create_cat_matrix(goodcat, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create reference graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_filepath = path.join(DATA_folder, \"precomp\", \"categories.dot\")\n",
    "def save_dot_file(names, cats, goodcats, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"graph graphname {\")\n",
    "        f.write(\"node [shape=point, fontsize=0]\")\n",
    "        for c in goodcats:\n",
    "            for i, a in enumerate(cats[c]):\n",
    "                for j in range(i + 1, len(cats[c])):\n",
    "                    x = names.index(cats[c][i])\n",
    "                    y = names.index(cats[c][j])\n",
    "                    if x < 200 and y < 200:\n",
    "                        f.write(\"a%d -- a%d;\" % (x, y))\n",
    "        f.write(\"}\")\n",
    "\n",
    "save_dot_file(names, cats, goodcat, graph_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1022.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cat_matrix[:100, :100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute Word2vec of each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2vec model.\n",
      "Loaded from pickle\n"
     ]
    }
   ],
   "source": [
    "Nvec = 300\n",
    "word2vec = wiki.get_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating vectorized pages\n",
      "Computed 2479 pages"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mulcyber/Projets/wikipediaNLP/wikipedia_preprocessing.py:648: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(Ncorp / appears)\n"
     ]
    }
   ],
   "source": [
    "filepath = path.join(DATA_folder, \"precomp\", \"pages_vectorize_cats.vec.gz\")\n",
    "pages = wiki.Pages()\n",
    "wiki.vectorize_pages_filtered(pages, word2vec, names, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: List vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = word2vec.index2entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compute inverse document frequency (idf) of each term\n",
    "\n",
    "Done during preprocessing of doc_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: For each document, compute each term frequency (tf) and tf-idf. Keep only the N=100 most important terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2479/2480"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "pages = wiki.Pages()\n",
    "idf_filepath = path.join(DATA_folder, \"precomp\", \"idf.pkl\")\n",
    "\n",
    "def compute_tf(d, idf):\n",
    "    tokens = word_tokenize(d[\"content\"])\n",
    "    n = len(tokens)\n",
    "    f = {}\n",
    "    terms = []\n",
    "    idf_t = []\n",
    "    for t in tokens:\n",
    "        if word2vec.vocab.get(t):\n",
    "            if t not in terms:\n",
    "                terms.append(t)\n",
    "                idf_t.append(idf[word2vec.vocab.get(t).index])\n",
    "                f[t] = 1\n",
    "                if idf_t[-1] == np.inf:\n",
    "                    print(\"IDF inf for %s\" % t)\n",
    "            else:\n",
    "                f[t] += 1         \n",
    "    terms = np.array(terms)\n",
    "    f = np.array([f[t] for t in terms])\n",
    "    f = 0.5 + 0.5 * f / np.max(f)\n",
    "    idf_t = np.array(idf_t)\n",
    "    return terms, f * idf_t\n",
    "\n",
    "def compute_doc_vectors():\n",
    "    D = wiki.Pages()\n",
    "    with open(idf_filepath, \"rb\") as f:\n",
    "        idf = pickle.load(f)\n",
    "    doc_vectors = np.zeros((Ncorp, N, Nvec + 1))\n",
    "    i = 0\n",
    "    for d in D:\n",
    "        if d[\"name\"] in names:\n",
    "            print(\"\\r%d/%d\" % (i, Ncorp), end=\"\")\n",
    "            terms, tfidf = compute_tf(d, idf)\n",
    "            index_sorted = (-tfidf).argsort()\n",
    "            terms = terms[index_sorted]\n",
    "            tfidf = tfidf[index_sorted]\n",
    "            for j in range(min(N, len(terms))):\n",
    "                doc_vectors[i, j, :-1] = word2vec.get_vector(terms[j])\n",
    "            doc_vectors[i, :min(N, len(terms)), -1] = tfidf[:N]\n",
    "            i += 1\n",
    "    return doc_vectors\n",
    "\n",
    "vec_doc_filepath = path.join(DATA_folder, \"vec_doc.pkl\")\n",
    "\n",
    "if path.isfile(vec_doc_filepath):\n",
    "    with open(vec_doc_filepath, \"rb\") as f:\n",
    "        doc_vectors = pickle.load(f)\n",
    "    print(\"Loaded doc vectors\")\n",
    "else:\n",
    "    doc_vectors = compute_doc_vectors()\n",
    "    with open(vec_doc_filepath, \"wb\") as f:\n",
    "        pickle.dump(doc_vectors, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: creating a search function using this similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(idf_filepath, \"rb\") as f:\n",
    "        idf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hearthstone', 1.0),\n",
       " ('fire-place', 0.6301341652870178),\n",
       " ('easy-chair', 0.6231460571289062),\n",
       " ('hearth', 0.613952100276947),\n",
       " ('tea-table', 0.6105866432189941),\n",
       " ('writing-table', 0.6100817322731018),\n",
       " ('court-yard', 0.5942029356956482),\n",
       " ('farm-house', 0.59410160779953),\n",
       " ('bed-room', 0.5908367037773132),\n",
       " ('livingroom', 0.5757289528846741)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_similar(name, sim_matrix, names, N=10):\n",
    "    matches = np.argsort(-sim_matrix[names.index(name), :])\n",
    "    return [names[matches[i]] for i in range(N)]\n",
    "\n",
    "print(search_similar('Albert Einstein', sim_matrix, names, N=10))\n",
    "print(search_similar('Apollo 11', sim_matrix, names, N=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(name, names, doc_vectors, N=10):\n",
    "    return [word2vec.similar_by_vector(doc_vectors[names.index(name), i, :Nvec])[0] for i in range(N)]\n",
    "    \n",
    "get_words('Albert Einstein', names, doc_vectors, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the graph\n",
    "\n",
    "### Step1: Compute all document similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_similarity1(d1, d2):\n",
    "    sim = 0\n",
    "    for v1 in d1[:N]:\n",
    "        t1 = v1[:Nvec]\n",
    "        for v2 in d2[:N]:\n",
    "            t2 = v2[:Nvec]\n",
    "            sim += cosine_similarity([t1], [t2]) / N**2\n",
    "    return sim\n",
    "\n",
    "\n",
    "def doc_similarity2(d1, d2):\n",
    "    sim = 0\n",
    "    for v1 in d1[:N]:\n",
    "        t1 = v1[:Nvec]\n",
    "        max_sim = 0\n",
    "        for v2 in d2[:N]:\n",
    "            t2 = v2[:Nvec]\n",
    "            curr_sim = cosine_similarity([t1], [t2])\n",
    "            if curr_sim > max_sim:\n",
    "                max_sim = curr_sim\n",
    "        sim += max_sim / N\n",
    "    return sim\n",
    "\n",
    "\n",
    "def doc_similarity3(d1, d2):\n",
    "    sim = 0\n",
    "    for v1 in d1[:N]:\n",
    "        t1 = v1[:Nvec]\n",
    "        tfidf1 = v1[-1]\n",
    "        for v2 in d2[:N]:\n",
    "            t2 = v2[:Nvec]\n",
    "            tfidf2 = v2[-1]\n",
    "            sim += cosine_similarity([t1], [t2]) * tfidf1 * tfidf2 / N**2\n",
    "    return sim\n",
    "\n",
    "\n",
    "def doc_similarity4(d1, d2):\n",
    "    sim = 0\n",
    "    for v1 in d1[:N]:\n",
    "        t1 = v1[:Nvec]\n",
    "        tfidf1 = v1[-1]\n",
    "        max_sim = 0\n",
    "        for v2 in d2[:N]:\n",
    "            t2 = v2[:Nvec]\n",
    "            tfidf2 = v2[-1]\n",
    "            curr_sim = cosine_similarity([t1], [t2]) * tfidf1 * tfidf2\n",
    "            if curr_sim > max_sim:\n",
    "                max_sim = curr_sim\n",
    "            sim += max_sim / N\n",
    "    return sim\n",
    "\n",
    "\n",
    "def similarity_matrix(doc_vec, sim_func):\n",
    "    Ncorp = len(doc_vec)\n",
    "    sim_matrix = np.zeros((Ncorp, Ncorp))\n",
    "    for i in range(Ncorp):\n",
    "        print(\"\\r%s articles computed\" % i, end=\"\")\n",
    "        for j in range(i):\n",
    "            sim_matrix[i, j] = sim_matrix[j, i] = sim_func(doc_vec[i], doc_vec[j])\n",
    "        if i and i % 10 == 0:\n",
    "            with open(sim_matrix_path % i, \"wb\") as f:\n",
    "                pickle.dump(sim_matrix[:i, :i], f)\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_path = path.join(DATA_folder, \"precomp\", \"sim1_small_matrix_%d.pkl\")\n",
    "N = 100\n",
    "Nvec = 300\n",
    "sim_matrix = similarity_matrix(doc_vectors[:201], doc_similarity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_path = path.join(DATA_folder, \"precomp\", \"sim2_matrix_%d.pkl\")\n",
    "sim_matrix = similarity_matrix(doc_vectors[:101], doc_similarity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 articles computed"
     ]
    }
   ],
   "source": [
    "sim_matrix_path = path.join(DATA_folder, \"precomp\", \"sim3_matrix_%d.pkl\")\n",
    "sim_matrix = similarity_matrix(doc_vectors[:101], doc_similarity3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_path = path.join(DATA_folder, \"precomp\", \"sim4_matrix_%d.pkl\")\n",
    "sim_matrix = similarity_matrix(doc_vectors[:101], doc_similarity4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_path = path.join(DATA_folder, \"precomp\", \"sim1_small_matrix_%d.pkl\")\n",
    "N = 20\n",
    "Nvec = 20\n",
    "sim_matrix = similarity_matrix(doc_vectors[:201], doc_similarity1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Compute similarity histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_path = path.join(DATA_folder, \"precomp\", \"sim1_matrix_%d.pkl\")\n",
    "\n",
    "with open(sim_matrix_path % 100, \"rb\") as f:\n",
    "    sim_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(sim_matrix[sim_matrix != 0].reshape((-1, 1)), 40)\n",
    "plt.xlabel(\"Similarity\")\n",
    "plt.ylabel(\"N\")\n",
    "plt.savefig(path.join(DATA_folder, \"img\", \"hist1_%d.png\" % len(sim_matrix)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Choose threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_pos_rate(ref_matrix, sim_matrix, threshold):\n",
    "    return np.sum(ref_matrix[sim_matrix > threshold]) / np.sum(ref_matrix)\n",
    "\n",
    "def false_pos_rate(ref_matrix, sim_matrix, threshold):\n",
    "    return np.sum(1 - ref_matrix[sim_matrix > threshold]) / np.sum(1 - ref_matrix)\n",
    "\n",
    "def RoC_curve(ref_matrix, sim_matrix, thresholds):\n",
    "    true_pos_rates = np.zeros(len(thresholds))\n",
    "    false_pos_rates = np.zeros(len(thresholds))\n",
    "    for i, t in enumerate(thresholds):\n",
    "        true_pos_rates[i] = true_pos_rate(ref_matrix, sim_matrix, t)\n",
    "        false_pos_rates[i] = false_pos_rate(ref_matrix, sim_matrix, t)\n",
    "    return true_pos_rates, false_pos_rates\n",
    "\n",
    "\n",
    "def plot_curve(ref_matrix, sim_matrix, a, b, n=20, filename=None):\n",
    "    thresholds = np.arange(a, b, (b - a)/n)\n",
    "    true_pos_rates, false_pos_rates = RoC_curve(ref_matrix, sim_matrix, thresholds)\n",
    "    f1 = 2 * true_pos_rates * (1 - false_pos_rates) / (true_pos_rates + 1 - false_pos_rates)\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(false_pos_rates, true_pos_rates)\n",
    "    ax2.plot(false_pos_rates, f1, \"r\")\n",
    "    ax1.plot([0, 1], [0, 1], \"b--\")\n",
    "    ax2.plot(false_pos_rates[np.argmax(f1)], np.max(f1), \"ro\")\n",
    "    ax2.annotate(\"t = %.2f\" % thresholds[np.argmax(f1)], (false_pos_rates[np.argmax(f1)], np.max(f1) + 0.03))\n",
    "    ax1.set_xlabel(\"FPR: False positive rate\")\n",
    "    ax1.set_ylabel(\"TPR: True positive rate\")\n",
    "    ax2.set_ylabel(\"F1 score\")\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax2.set_ylim([0, 1])\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix[sim_matrix == 0] = np.min(sim_matrix[sim_matrix != 0])\n",
    "Nmat = len(sim_matrix)\n",
    "ref_matrix = cat_matrix[:Nmat, :Nmat]\n",
    "a, b = -0.10, 0.15\n",
    "plot_curve(ref_matrix, sim_matrix, a, b, n=100, filename=path.join(DATA_folder, \"img\", \"ROC1_small_%d.png\" % len(sim_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_graph_filepath = path.join(DATA_folder, \"precomp\", \"similarity200.dot\")\n",
    "def sim_save_dot_file(matrix, threshold, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        N = len(matrix)\n",
    "        f.write(\"graph graphname {\")\n",
    "        f.write(\"node [shape=point, fontsize=0]\")\n",
    "        for i in range(N):\n",
    "            for j in range(i + 1, N):\n",
    "                if matrix[i, j] > threshold:\n",
    "                    f.write(\"a%d -- a%d;\" % (i, j))\n",
    "        f.write(\"}\")\n",
    "\n",
    "sim_save_dot_file(sim_matrix, 42, sim_graph_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Graph representation\n",
    "\n",
    "Done using the sdpf command of graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision to wikipedia portals\n",
    "\n",
    "See RoC curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
